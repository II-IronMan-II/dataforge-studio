from __future__ import annotations

TEMPLATES: dict[str, dict[str, str]] = {
    "trim": {
        "snowflake_sql": "TRIM({col})",
        "spark_sql":     "TRIM({col})",
        "bigquery_sql":  "TRIM({col})",
        "tsql":          "LTRIM(RTRIM({col}))",
        "mysql":         "TRIM({col})",
        "postgresql":    "TRIM({col})",
        "ansi":          "TRIM({col})",
        "pyspark":       "F.trim(F.col('{col}'))",
        "python":        "{col}.strip()",
    },
    "case_upper": {
        "snowflake_sql": "UPPER({col})",
        "spark_sql":     "UPPER({col})",
        "bigquery_sql":  "UPPER({col})",
        "tsql":          "UPPER({col})",
        "mysql":         "UPPER({col})",
        "postgresql":    "UPPER({col})",
        "ansi":          "UPPER({col})",
        "pyspark":       "F.upper(F.col('{col}'))",
        "python":        "{col}.upper()",
    },
    "case_lower": {
        "snowflake_sql": "LOWER({col})",
        "spark_sql":     "LOWER({col})",
        "bigquery_sql":  "LOWER({col})",
        "tsql":          "LOWER({col})",
        "mysql":         "LOWER({col})",
        "postgresql":    "LOWER({col})",
        "ansi":          "LOWER({col})",
        "pyspark":       "F.lower(F.col('{col}'))",
        "python":        "{col}.lower()",
    },
    "case_title": {
        "snowflake_sql": "INITCAP({col})",
        "spark_sql":     "INITCAP({col})",
        "bigquery_sql":  "INITCAP({col})",
        "tsql":          "__WARNING__:T-SQL has no native INITCAP. Use custom logic or a CLR function.",
        "mysql":         "__WARNING__:MySQL has no native INITCAP. Use custom logic.",
        "postgresql":    "INITCAP({col})",
        "ansi":          "__WARNING__:ANSI SQL has no native INITCAP.",
        "pyspark":       "F.initcap(F.col('{col}'))",
        "python":        "{col}.title()",
    },
    "null_replace": {
        "snowflake_sql": "COALESCE({col}, '{replacement}')",
        "spark_sql":     "COALESCE({col}, '{replacement}')",
        "bigquery_sql":  "COALESCE({col}, '{replacement}')",
        "tsql":          "ISNULL({col}, '{replacement}')",
        "mysql":         "COALESCE({col}, '{replacement}')",
        "postgresql":    "COALESCE({col}, '{replacement}')",
        "ansi":          "COALESCE({col}, '{replacement}')",
        "pyspark":       "F.coalesce(F.col('{col}'), F.lit('{replacement}'))",
        "python":        "{col} if {col} is not None else '{replacement}'",
    },
    "null_drop": {
        "snowflake_sql": "{col} IS NOT NULL",
        "spark_sql":     "{col} IS NOT NULL",
        "bigquery_sql":  "{col} IS NOT NULL",
        "tsql":          "{col} IS NOT NULL",
        "mysql":         "{col} IS NOT NULL",
        "postgresql":    "{col} IS NOT NULL",
        "ansi":          "{col} IS NOT NULL",
        "pyspark":       "F.col('{col}').isNotNull()",
        "python":        "{col} is not None",
    },
    "null_flag": {
        "snowflake_sql": "CASE WHEN {col} IS NULL THEN 1 ELSE 0 END",
        "spark_sql":     "CASE WHEN {col} IS NULL THEN 1 ELSE 0 END",
        "bigquery_sql":  "CASE WHEN {col} IS NULL THEN 1 ELSE 0 END",
        "tsql":          "CASE WHEN {col} IS NULL THEN 1 ELSE 0 END",
        "mysql":         "IF({col} IS NULL, 1, 0)",
        "postgresql":    "CASE WHEN {col} IS NULL THEN 1 ELSE 0 END",
        "ansi":          "CASE WHEN {col} IS NULL THEN 1 ELSE 0 END",
        "pyspark":       "F.when(F.col('{col}').isNull(), F.lit(1)).otherwise(F.lit(0))",
        "python":        "1 if {col} is None else 0",
    },
    "cast_string": {
        "snowflake_sql": "CAST({col} AS VARCHAR)",
        "spark_sql":     "CAST({col} AS STRING)",
        "bigquery_sql":  "SAFE_CAST({col} AS STRING)",
        "tsql":          "CAST({col} AS NVARCHAR(MAX))",
        "mysql":         "CAST({col} AS CHAR)",
        "postgresql":    "CAST({col} AS TEXT)",
        "ansi":          "CAST({col} AS VARCHAR)",
        "pyspark":       "F.col('{col}').cast('string')",
        "python":        "str({col})",
    },
    "cast_integer": {
        "snowflake_sql": "CAST({col} AS INTEGER)",
        "spark_sql":     "CAST({col} AS INTEGER)",
        "bigquery_sql":  "SAFE_CAST({col} AS INT64)",
        "tsql":          "CAST({col} AS INT)",
        "mysql":         "CAST({col} AS SIGNED INTEGER)",
        "postgresql":    "CAST({col} AS INTEGER)",
        "ansi":          "CAST({col} AS INTEGER)",
        "pyspark":       "F.col('{col}').cast('integer')",
        "python":        "int({col})",
    },
    "cast_float": {
        "snowflake_sql": "CAST({col} AS FLOAT)",
        "spark_sql":     "CAST({col} AS DOUBLE)",
        "bigquery_sql":  "SAFE_CAST({col} AS FLOAT64)",
        "tsql":          "CAST({col} AS FLOAT)",
        "mysql":         "CAST({col} AS DECIMAL(18,4))",
        "postgresql":    "CAST({col} AS DOUBLE PRECISION)",
        "ansi":          "CAST({col} AS DECIMAL)",
        "pyspark":       "F.col('{col}').cast('double')",
        "python":        "float({col})",
    },
    "cast_date": {
        "snowflake_sql": "TRY_TO_DATE({col}, '{format}')",
        "spark_sql":     "TO_DATE({col}, '{format}')",
        "bigquery_sql":  "SAFE.PARSE_DATE('{format}', {col})",
        "tsql":          "TRY_CONVERT(DATE, {col})",
        "mysql":         "STR_TO_DATE({col}, '{format}')",
        "postgresql":    "TO_DATE({col}, '{format}')",
        "ansi":          "CAST({col} AS DATE)",
        "pyspark":       "F.to_date(F.col('{col}'), '{format}')",
        "python":        "datetime.strptime({col}, '{format}').date()",
    },
    "cast_timestamp": {
        "snowflake_sql": "TRY_TO_TIMESTAMP({col})",
        "spark_sql":     "TO_TIMESTAMP({col})",
        "bigquery_sql":  "SAFE_CAST({col} AS TIMESTAMP)",
        "tsql":          "TRY_CONVERT(DATETIME2, {col})",
        "mysql":         "STR_TO_DATE({col}, '%Y-%m-%d %H:%i:%s')",
        "postgresql":    "CAST({col} AS TIMESTAMP)",
        "ansi":          "CAST({col} AS TIMESTAMP)",
        "pyspark":       "F.to_timestamp(F.col('{col}'))",
        "python":        "datetime.fromisoformat({col})",
    },
    "strip_special": {
        "snowflake_sql": "REGEXP_REPLACE({col}, '[^a-zA-Z0-9 ]', '')",
        "spark_sql":     "REGEXP_REPLACE({col}, '[^a-zA-Z0-9 ]', '')",
        "bigquery_sql":  "REGEXP_REPLACE({col}, '[^a-zA-Z0-9 ]', '')",
        "tsql":          "__WARNING__:T-SQL has no native REGEXP_REPLACE. Use CLR function or handle in application layer.",
        "mysql":         "REGEXP_REPLACE({col}, '[^a-zA-Z0-9 ]', '')",
        "postgresql":    "REGEXP_REPLACE({col}, '[^a-zA-Z0-9 ]', '', 'g')",
        "ansi":          "__WARNING__:ANSI SQL has no native REGEXP_REPLACE for strip_special.",
        "pyspark":       "F.regexp_replace(F.col('{col}'), '[^a-zA-Z0-9 ]', '')",
        "python":        "re.sub('[^a-zA-Z0-9 ]', '', {col})",
    },
    "regex_replace": {
        "snowflake_sql": "REGEXP_REPLACE({col}, '{pattern}', '{repl}')",
        "spark_sql":     "REGEXP_REPLACE({col}, '{pattern}', '{repl}')",
        "bigquery_sql":  "REGEXP_REPLACE({col}, '{pattern}', '{repl}')",
        "tsql":          "__WARNING__:T-SQL has no native REGEXP_REPLACE. Use CLR function or handle in application layer.",
        "mysql":         "REGEXP_REPLACE({col}, '{pattern}', '{repl}')",
        "postgresql":    "REGEXP_REPLACE({col}, '{pattern}', '{repl}', 'g')",
        "ansi":          "__WARNING__:ANSI SQL has no native REGEXP_REPLACE.",
        "pyspark":       "F.regexp_replace(F.col('{col}'), '{pattern}', '{repl}')",
        "python":        "re.sub('{pattern}', '{repl}', {col})",
    },
    "where_filter": {
        "snowflake_sql": "CASE WHEN {condition} THEN {col} ELSE NULL END",
        "spark_sql":     "CASE WHEN {condition} THEN {col} ELSE NULL END",
        "bigquery_sql":  "CASE WHEN {condition} THEN {col} ELSE NULL END",
        "tsql":          "CASE WHEN {condition} THEN {col} ELSE NULL END",
        "mysql":         "CASE WHEN {condition} THEN {col} ELSE NULL END",
        "postgresql":    "CASE WHEN {condition} THEN {col} ELSE NULL END",
        "ansi":          "CASE WHEN {condition} THEN {col} ELSE NULL END",
        "pyspark":       "F.when(F.expr('{condition}'), F.col('{col}')).otherwise(None)",
        "python":        "{col} if ({condition}) else None",
    },
    "conditional_case": {
        "snowflake_sql": "CASE {cases} ELSE '{else_value}' END",
        "spark_sql":     "CASE {cases} ELSE '{else_value}' END",
        "bigquery_sql":  "CASE {cases} ELSE '{else_value}' END",
        "tsql":          "CASE {cases} ELSE '{else_value}' END",
        "mysql":         "CASE {cases} ELSE '{else_value}' END",
        "postgresql":    "CASE {cases} ELSE '{else_value}' END",
        "ansi":          "CASE {cases} ELSE '{else_value}' END",
        "pyspark":       "__WARNING__:PySpark conditional requires chained F.when() calls; generate manually.",
        "python":        "__WARNING__:Python conditional requires manual if/elif chain.",
    },
    "delimiter_split": {
        "snowflake_sql": "SPLIT_PART({col}, '{delimiter}', {index})",
        "spark_sql":     "SPLIT({col}, '{delimiter}')[{index}]",
        "bigquery_sql":  "SPLIT({col}, '{delimiter}')[SAFE_OFFSET({index})]",
        "tsql":          "__WARNING__:T-SQL has no native SPLIT_PART. Use STRING_SPLIT or handle in application layer.",
        "mysql":         "SUBSTRING_INDEX(SUBSTRING_INDEX({col}, '{delimiter}', {index}+1), '{delimiter}', -1)",
        "postgresql":    "SPLIT_PART({col}, '{delimiter}', {index}+1)",
        "ansi":          "__WARNING__:ANSI SQL has no native SPLIT_PART.",
        "pyspark":       "F.split(F.col('{col}'), '{delimiter}').getItem({index})",
        "python":        "{col}.split('{delimiter}')[{index}]",
    },
}

TRANSFORM_ORDER: list[str] = [
    "trim",
    "strip_special",
    "regex",
    "case_normalization",
    "type_cast",
    "null_strategy",
    "where_filter",
    "conditional",
    "delimiter_split",
]
